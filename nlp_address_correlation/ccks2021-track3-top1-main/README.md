# ccks2021-track3
CCKS2021中文NLP地址相关性任务-赛道三-冠军方案

团队：我的加菲鱼-  wodejiafeiyu

初赛第二/复赛第一/决赛第一  
## 前言

19年开始，陆陆续续参加了一些比赛，拿到过一些top，比较懒一直都没分享过，这次比较幸运又拿了top1，打算分享下

分类的任务用这个框架基本都能top10，之前个人参加的《全球人工智能大赛-赛道一》和这个也差不多，只是有些具体任务的trick不同

## 比赛主页
https://tianchi.aliyun.com/competition/entrance/531901/introduction

## 环境
* torch==1.6.0
* transformers=3.0.2



## 预训练模型
    - nezha-base 
    - nezha-wwm
    - macbert
* 下载预训练模型，放到文件夹user_data/model_param/pretrain_model_param/下，文件夹和模型名字一一对应


## 全流程脚本
```
sh run.sh
```

## 核心思路

*  预训练mlm中的mask策略使用ngram-mask，相比原始的动态mask，提升了预训练难度
*  标签也包含语义信息，预训练部分融入标签信息，提升预训练效果
*  混合精度预训练，损失一部分精度，提升整体的训练速度，实际测试结果精度损失不大，速度提升明显
*  对抗训练，已经是一个比较常用的trick了
*  后12层加上embedding的cls动态加权平均
*  multi-sample dropout
*  推理的时候，阈值搜索
*  三折交叉验证，每折使用不同的随机数种子使用dynamic pad
*  加权平均，模型融合

## 总结
| 模块    |                  提分点分析              | 提升    | 
|------   |-----------------------------------------| ------- | 
|ngram-mask  |相比于单个字的遮蔽，ngram-mask加大了预训练任务的难度，从而提升效果| 2.6个千分点 | 
|融入标签信息预训练|标签也含有语义信息，模型学习的更多| 1个千分点   |
|后12层加上embedding的cls动态加权平均+multi-sample dropout|加权平均能增强向量的语义表征能力从而提升效果。multi-sample dropout能加速训练，增强泛化能力| 3.3个千分点   | 
|  对抗训练fgm  |生成对抗样本，对抗样本的训练，增加模型泛化能力 | 2.2个千分点   | 
|  多分类阈值搜索   |       缩放系数使得f1最优      | 0.7个千分点   | 
|  模型融合  |          提升模型的鲁棒性和泛化能力            | 1.5个千分点  |
|  动态pad和预训练混合精度训练和每折使用不同的随机数种子 |     提升模型训练速度，不同的随机数种子加大了模型的差异，提升融合的效果     |  | 


